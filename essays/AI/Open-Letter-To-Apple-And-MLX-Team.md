# Open Letter to Apple and MLX Team

❗️Please note: This letter is a response to the request for feedback on the MLX project. I've chosen to share my thoughts publicly in the hope that this feedback is taken in the spirit of constructive discussion.

https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/essays/AI/Open-Letter-To-Apple-And-MLX-Team.md

I'll share my take without beating around the bush.

I've been a loyal Apple user for over a decade, and an enthusiast for much longer.

But let's see what happens when I try to use MLX. Thinking "You are Apple."

Consider this from a non-Apple user’s point of view. Someone who is not a geek, nerd, or technically savvy. Let's call him Tenny.

Tenny has just stepped into the AI and ML arena, filled with excitement and eagerness to explore this new frontier of AI.

He even buys an M3 Max, filled with anticipation. "Wow, Apple has introduced something named MLX. It should match PyTorch's coolness. After all, this is Apple."

He embarks on a small project and checks out your documentation. He searches for 'ReLU'.

![torch-relu.png](images%2Ftorch-relu.png)

PyTorch offers a clear piece of documentation, as shown in the screenshot. The presence of 'Meta' is unmistakable in the document.

![mlx-relu.png](images%2Fmlx-relu.png)

However, MLX presents a totally different narrative.

Apple's presence is nowhere to be found in all this. If he didn’t know MLX was an Apple product, he'd think it's a framework by some obscure startup. Last time he checked, Apple is the most valuable company in the world. "YOU ARE APPLE."

![mlx-tutorial.png](images%2Fmlx-tutorial.png)

Perplexed, he experiments with other examples. The story remains the same. Tutorials? Almost non-existent. Not what one would anticipate from giants like Meta and Apple.

![torch-tutorials.png](images%2Ftorch-tutorials.png)

Look at the PyTorch tutorials. They epitomize richness.

I recognize your hard work and value that MLX is an emerging framework. Yet, most users won’t be as patient and understanding as I am.

Even mid-sized open-source projects usually have sufficient documentation. Why? Because they eagerly want to attract users. They want to grow. They want to be recognized. But Tenny wouldn't sense that eagerness from MLX.

For example, I effortlessly got my LLM projects up and running using LangChain documentation without posing a single question to the developers or the community. LangChain should be a really small endeaver compared to `Apple MLX`. Yet, the documentation is richer and clearer. We should rightfully expect more from Apple, right? "YOU ARE APPLE."

In some MLX examples, it seems that even Apple developers themselves are a bit uncertain about MLX best practices. An MLX developer directed me to the mlx-data repo when I asked him about MLX way of handling data. So why include the Dataset wrapper and batchyfier function in the LoRA example then? This leads to the assumption that we could handle it like PyTorch. Just extend the Dataset class, initiate a DataLoader, and that’s it. Yet, there's this entirely new 'mlx-data', and you expect us to decipher everything on our own? How would Tenny react? Would he initially realize that he should explore 'mlx-data' with little to no documentation and support from Apple? "YOU ARE APPLE."

Most of us are consumers, not your developers or testers.

Playing with your examples is one thing, deciphering them and building something from scratch is another. 

Faced with these challenges, Tenny would likely give up and turn to PyTorch. Honestly? I would advise him to do so. "YOU ARE APPLE."

Why try to reinvent the wheel when you have an already functional one right there? Versatile, resilient, multipurpose. As mentioned, I own a dozen Apple (including Intel Macs) and Windows machines, and PyTorch runs seamlessly on all of them. My Git repos in Dropbox sync perfectly. I can work on any device, anywhere, any time. Tenny would discover this. He'd also learn that PyTorch is much more mature and stable than MLX, even on Apple Silicon.

You can certainly position MLX as a competitive alternative to PyTorch on Apple Silicon, leveraging your home advantage. Tenny and I would then thoughtfully weigh the trade-offs. Are the potential advantages enough to justify the effort involved? Is tackling the steep learning curve and the challenge of essentially reinventing the wheel really worthwhile? Is Apple providing sufficient support to MLX to make it a viable alternative? For now, Tenny and I would likely conclude that the answer is no. "YOU ARE APPLE."

You need to intensify your efforts in promoting MLX. When I say 'you', I’m not referring to the MLX team, but to Apple.

A persistent question in my mind is, "How many of you at Apple are actually dedicated to MLX in this pivotal era of AI?"

Why is your team stretched so thin? Shouldn't Apple be expanding its workforce to focus on MLX? Given the importance of AI, isn't it crucial for Apple to demonstrate a stronger commitment to it?

Tenny wouldn't sense that commitment in all this. Honestly, I don't either, at least not yet. Please convince me otherwise. If convinced, I might even reconsider rebalancing my AI portfolio in the US market. Seriously.
For more on my honest take on MLX, check out this essay:

"I Have Good Wheels, You Know: Sticking to What Works"
https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/essays/computing/I-Have-Good-Wheels-Sticking-To-What-Works.md

Please Apple, consider Tenny’s perspective. It will guide you to a clearer path.

Thanks for your time.

#deeplearning
#machinelearning
#apple
#mlx
#pytorch
#jax
#tensorflow