# ðŸ’Ž The Dangers of Rewarding Faulty Models

One effective way to enhance AI's learning is to reward good outcomes and penalize errors. This ensures the model learns and avoids mistakes.

But what about humans? Remember, AI models are designed to replicate the most rational and ideal human brain, not just any average one.

An ideal, rational, and wise human would behave in the same manner.

So, if AI strives to emulate the best of us, why do some reward their own mistakes? Such actions suggest a deviation from the ideal that AI aims for.

For instance, claiming 200% confidence, which is statistically nonsensical, indicates that you are rewarding your poor habits instead of penalizing them.

Using leverage with a negative long-term average return is a self-evident misstep. It only magnifies losses:

    -1 x 2 = -2
    
    -1 x 10 = -10

This is akin to multiplying larger numbers with negatives, yielding bigger negative outcomes. By rewarding bad habits, your brain will fail to learn, and you are doomed to repeat the same mistakes over and over again, much like flawed regression-only AI models.

The real concern is the continuous reinforcement of poor decisions. Would anyone praise this model?

    Training accuracy: 0.0002
    
    Validation accuracy: 0.00001
    
    Test accuracy: 0.00000001

It's clear: a fresh start is needed. This model is unsalvageable.

