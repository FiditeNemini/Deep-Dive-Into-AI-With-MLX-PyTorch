# ðŸ’ŽA Path to Perfection - AI vs. Human

![the-debugger-01.jpeg](../images/the-debugger-01.jpeg)

We all make mistakes. AIs make mistakes. AGIs will make mistakes. Even God, some believe, allows for mistakes. Imperfection is seen as a gift from God, the Creator. You might be wondering why AGI or God aren't perfect. 

Perfection leaves no room for growth, which, in turn, means little meaning for existence. It's like a game already played to its fullest or a movie already seen with all its turns and twists. Sure, you can experience such entertainment, but the enjoyment is diminished. 

In that regard, if they claim they are perfect, they're not truly AGI or God. They should know they shouldn't be perfect. 

Divine perfection may mean the capacity to create and engage with a dynamic world, but such a concept is beyond the realm of human comprehension anyway.

We work towards perfection, a vanishing point which makes life rich and fun. If your life is already perfect, there would be no point in living it to its fullest.

How do we work towards that vanishing point of perfection?

Just like AI models do: through trial and error.

We seek truth and predict something to be true, and measure the distance between the ground truth and our prediction. If the distance is zero, it's a perfect prediction. Sometimes we make such perfect predictions but mostly we're wide off the mark. What's important is the long-term average: overall accuracy.

AIs learn (are trained on large data) to optimize the errors and raise their prediction accuracy by seeking the global minimum or maximum of the loss function where the gradient approaches 0. Zero means just that: zero costs in prediction or zero loss. That's why it is called either a cost or loss function.

Here's how AI models optimize their learning by reducing errors. Mathematically, they use differentiation to seek the lowest points in the given graph where you can draw horizontal lines tangent to the graph. The process is called 'gradient descent'. In mathematics, a gradient tells us the slope or steepness of a curve at any point. Imagine you're looking at a mountain â€“ the gradient at any point on the mountain's surface tells you how steep it is right there. In the context of a problem we're trying to solve, the "gradient" shows us how steeply the value we care about (like error or cost) changes as we adjust our variables. The word "descent" is used because we're trying to go down this metaphorical hill. Our goal is to reduce something, typically an error or cost. So, we're descending towards the lowest point, where this value is minimized.

(Note that we sometimes go up the hill, but the concept doesn't change. In gradient descent, the primary goal is to go downhill towards the lowest point. However, sometimes the process might momentarily go uphill, or in a direction that seems counterintuitive due to local minima, step sizes, and high-dimensional spaces, all of which indicate we're facing very complex problems to solve.)

Put together, "gradient descent" is about moving downwards (descent) in the direction that has the steepest slope (gradient), which leads us to the lowest point, or the optimal solution for our problem.

In plain English, gradient descent is like finding the lowest point in a valley. Imagine you're blindfolded in a hilly area, and your goal is to find the lowest point. You feel the ground under your feet to decide which way is downhill. You take a step in that direction, and then feel around again. You keep moving downhill, step by step. Eventually, you'll reach the lowest point in the valley. 

We're trying to minimize the distance between the ground truth we're seeking and the prediction we're making. We need gradient descent. 

(Also note that gradient descent is just one method of optimization used in machine learning. There are other methods, especially for certain types of problems or data sets, where gradient descent might not be the most efficient or effective.)

Some might err in concluding that the faster a model learns, the better. No, an optimized learning rate is crucial. Haste makes waste in the learning process. 

Normally, well-trained models reach 80 to 90% accuracy or 20 to 10% loss. If a model is almost perfect in its prediction, we suspect overfitting: the model is only good with the training 'already seen' data, it performs poorly with real 'unseen' data.  

Conversely, underfitting is a term used in machine learning to describe a model that performs poorly, not only on new, unseen data but also on the training data. This usually happens when the model is too simple or when it hasn't been trained long enough to learn the underlying patterns in the data. Underfitting can also occur when there isn't enough data for the model to learn from, or when the quality of the data is poor. 

In essence, underfitting is a sign that the model is unable to capture the underlying structure of the data, leading to inaccurate predictions and poor performance overall. It's a common challenge in machine learning, and addressing it often involves gathering more high-quality data, increasing the complexity of the model, or adjusting the training process.

Don't forget, for AI training, you need not just data, but a massive amount of it, along with substantial compute. It's a common human tendency to strive for perfection with as little training data as possible. This, however, is a flawed approach. 

Once again, it's important to draw parallels between AIs and humans. If an individual is claiming perfection, it's likely that they're demonstrating characteristics of overfitting or underfitting in their personal growth or learning process.

To measure the risks of overfitting and underfitting, we split large data into three portions: train, validation, and test sets. The key insight here is whether you're also good at unseen data. Anyone can be good with already seen data. The real test should be with real unseen data.

Uncharted territories come at a dear cost. Even almost perfect scored models may perform miserably in those territories. That's the reason well-received models like GPT-4 should be constantly monitored, scaled, and adjusted even after deployment.

We, humans, are just a model crafted by the Creator. We are designed just like that, to work towards the global minimum or maximum through trials and errors. I'm pretty positive the Creator himself or herself is pulling their hair out while debugging or optimizing human behaviors.

Despite the inherent complexities and differences between human and artificial intelligence, AIs are fundamentally modeled after humans in a broader sense, especially in terms of growth via trial and error. This concept of learning reflects a key aspect of human experience, where we learn and adapt through our interactions with the world. However, it's important to note that AIs are often designed to emulate an idealized version of human rationality and decision-making processes, rather than the unpredictable and sometimes erratic nature of actual human behavior. This distinction is crucial; the 'ideal' human that AI models aspire to emulate represents a more consistent and predictable pattern of rationality, which is easier to simulate and predict in an AI context. In this way, AIs can be seen as striving towards an optimized version of human-like learning and decision-making, one that is free from the whims and inconsistencies that often characterize our day-to-day human interactions.

In the realm of AI, decisions and predictions are inherently grounded in statistics and probabilities. This probabilistic approach mirrors a larger truth about our universe and the very fabric of our lives. Just as AI navigates through a landscape of probabilities, so too do we journey through life, where certainty is a rarity, and possibilities abound. Here, the metaphorical 'Principle of Uncertainty' reigns supreme, not just as a scientific concept, but as a philosophical truth. It suggests that our future is not preordained but is instead shaped by the myriad choices we make, each one steering us down a path of endless potential outcomes. This principle, in its broader interpretation, celebrates the unpredictability and dynamic nature of existence, reminding us that in both AI and life, the only constant is change and the opportunity it brings.

Engage in a conversation with your GPT and observe its operations: it's employing probabilistic models to logically predict the next word in the sequence. But don't be misled into thinking that this is just a mere predictive mechanism. Intriguingly, human communication shares a similar underlying complexity. In both humans and AIs, the process of generating and understanding language is steeped in layers of probability and contextual inference, though the full mechanics remain somewhat elusive in both realms.

Understanding AI is not just a technical endeavor; it's a journey towards deeper insights. If the significance of delving into the world of AI eludes you, consider this: AI offers a window into understanding not just advanced technology but also the intricacies of human cognition and communication. This exploration, I assert with confidence, can lead you to profound realizations and pathways, perhaps those you've been seeking throughout your life. Embracing AI is more than learning about algorithms; it's about uncovering the patterns that govern our world and our very existence.

The journey towards perfection is, paradoxically, an imperfect one, rich with unexpected joys and valuable discoveries. This path, strewn with both challenges and triumphs, offers a dynamic and rewarding adventure. Embrace this journey with enthusiasm and an open heart, for the true treasures lie not in the destination, but in the experiences gathered along the way. 

Now, go forth and revel in this exciting quest!
